{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b591e6e2-861a-40b0-a612-398c7d06af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71cc35aa-a0d0-40cd-94bc-985364bca6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data sets\n",
    "\n",
    "X_train = np.random.uniform(-1, 1, (5, 20)) #25, 20, 5\n",
    "Y_train = np.random.uniform(0, 1, (2, 20)) # 25, 20, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2ae7711-e3a2-446d-b1a8-9902e30f72dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the class FFN:\n",
    "# a time stepper that takes in (input_data, output_data, hidden_layers (list), activation_function (list of tuples), epochs, learning rate)\n",
    "# going to assume mean squared error and stochastic gradient descent.\n",
    "class FFN:\n",
    "    def __init__(self, input_data, output_data, hidden_layers, activation_functions, epochs, learning_rate):\n",
    "        self.input_data = input_data ## make sure this is a numpy array\n",
    "        self.output_data = output_data\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activation_functions = activation_functions\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights_dictionary = {}\n",
    "        self.activations_dictionary = {}\n",
    "        self.initialize()\n",
    "\n",
    "\n",
    "    def initialize(self):\n",
    "        last_layer = self.input_data.shape[0]\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            self.weights_dictionary[f'w{i}'] = 0.1 * np.random.randn(self.hidden_layers[i], last_layer)\n",
    "            self.weights_dictionary[f'b{i}'] = np.zeros((self.hidden_layers[i], 1))\n",
    "            last_layer = self.hidden_layers[i]\n",
    "        self.weights_dictionary['last_weight'] = 0.1 * np.random.randn(self.output_data.shape[0], last_layer)\n",
    "        self.weights_dictionary['last_bias'] = np.zeros((self.output_data.shape[0], 1))\n",
    "        \n",
    "                \n",
    "    def forward(self, input_batch): #setting up to be called within the training method for each batch\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            z = np.dot(self.weights_dictionary[f'w{i}'], input_batch) + self.weights_dictionary[f'b{i}'] # so weights of first layer are \n",
    "            print(f' input {input_batch.shape}', f\"biases {self.weights_dictionary[f'b{i}'].shape}\", f\"weights {self.weights_dictionary[f'w{i}'].shape}\")\n",
    "            a = self.activation(z, self.activation_functions[i]) # should have shape (neurons in layer 1, batch size)\n",
    "            self.activations_dictionary[f'z{i + 1}'] = z #activation (prefunction) of the first hidden layer is added as z1 \n",
    "            self.activations_dictionary[f'a{i + 1}'] = a #activation of first layer added as a1\n",
    "            input_batch = a\n",
    "        print(f'weights {self.weights_dictionary[\"last_weight\"].shape}', f'input {input_batch.shape}', f'biases {self.weights_dictionary[\"last_bias\"].shape}')   \n",
    "        a = np.dot(self.weights_dictionary['last_weight'], input_batch) + self.weights_dictionary[\"last_bias\"]\n",
    "        self.activations_dictionary[f'activation_output'] = a\n",
    "        ## so much cleaner instead of having to index a load of lists and potentially reverse them etc.\n",
    "        \n",
    "    def activation(self, a, activation_function):\n",
    "        if activation_function[0] == \"relu\":\n",
    "            return np.maximum(0, a)\n",
    "        elif activation_function[0] == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-a))\n",
    "        elif activation_function[0] == \"tanh\":\n",
    "            return np.tanh(a)\n",
    "        elif activation_function[0] == \"lrelu\": # THink about how you can have an alpha parameter when you want it \n",
    "            return np.maximum(activation_function[1] * a, a) \n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ae97d0d-0690-4dbe-a825-575b531161b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input (5, 20) biases (30, 1) weights (30, 5)\n",
      " input (30, 20) biases (30, 1) weights (30, 30)\n",
      "weights (2, 30) input (30, 20) biases (2, 1)\n"
     ]
    }
   ],
   "source": [
    "tester = FFN(X_train, Y_train, [30, 30], [(\"relu\", 0), (\"relu\", 0)], 10, 0.1)\n",
    "tester.forward(X_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
